{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LumR3epEAkaJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **What is Simple Linear Regression?**\n",
        "   A regression method that models the relationship between one independent variable (**X**) and one dependent variable (**Y**) using a straight line.\n",
        "\n",
        "2. **Key assumptions of Simple Linear Regression:**\n",
        "\n",
        "   * Linearity between X and Y\n",
        "   * Independence of residuals\n",
        "   * Homoscedasticity (constant variance of residuals)\n",
        "   * Normality of residuals\n",
        "\n",
        "3. **What does the coefficient `m` represent in `Y = mX + c`?**\n",
        "   It is the **slope** of the line — change in Y for a one-unit increase in X.\n",
        "\n",
        "4. **What does the intercept `c` represent in `Y = mX + c`?**\n",
        "   The value of Y when X = 0.\n",
        "\n",
        "5. **How do we calculate the slope `m` in Simple Linear Regression?**\n",
        "\n",
        "   $$\n",
        "   m = \\frac{\\sum{(X_i - \\bar{X})(Y_i - \\bar{Y})}}{\\sum{(X_i - \\bar{X})^2}}\n",
        "   $$\n",
        "\n",
        "6. **Purpose of the least squares method in SLR:**\n",
        "   To minimize the **sum of squared residuals** (errors) between observed and predicted Y values.\n",
        "\n",
        "7. **How is R² interpreted in SLR?**\n",
        "   Proportion of variance in Y explained by X.\n",
        "   R² = 0.85 → 85% of variation in Y is explained by X.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "8. **What is Multiple Linear Regression?**\n",
        "   A regression model where the dependent variable is predicted using **two or more** independent variables.\n",
        "\n",
        "9. **Main difference between SLR and MLR:**\n",
        "   SLR uses **one** predictor; MLR uses **multiple** predictors.\n",
        "\n",
        "10. **Key assumptions of MLR:**\n",
        "\n",
        "    * Linearity\n",
        "    * Independence\n",
        "    * Homoscedasticity\n",
        "    * No multicollinearity\n",
        "    * Normal distribution of errors\n",
        "\n",
        "11. **What is heteroscedasticity?**\n",
        "    Unequal spread of residuals across the range of predictors. It violates model assumptions and affects standard errors and inference.\n",
        "\n",
        "12. **How to improve MLR with high multicollinearity:**\n",
        "\n",
        "    * Remove/reduce correlated features\n",
        "    * Use **Ridge/Lasso regression**\n",
        "    * Use **PCA** or **feature selection**\n",
        "\n",
        "13. **Common techniques for transforming categorical variables:**\n",
        "\n",
        "    * **One-hot encoding**\n",
        "    * **Label encoding**\n",
        "    * **Ordinal encoding** (if categories have order)\n",
        "\n",
        "14. **Role of interaction terms in MLR:**\n",
        "    To capture combined effects of variables (e.g., X1 \\* X2) that may influence the outcome differently when used together.\n",
        "\n",
        "15. **Interpretation of intercept (SLR vs MLR):**\n",
        "\n",
        "    * SLR: Y when X = 0\n",
        "    * MLR: Y when **all X variables = 0** (may not be meaningful if unrealistic)\n",
        "\n",
        "16. **Significance of slope:**\n",
        "    It quantifies the relationship — **how much Y changes** per unit increase in X (holding others constant in MLR).\n",
        "\n",
        "17. **How intercept provides context:**\n",
        "    It anchors the line; helps define the baseline value of Y when all predictors are zero.\n",
        "\n",
        "18. **Limitations of R²:**\n",
        "\n",
        "    * Doesn’t indicate causation\n",
        "    * Increases with more variables (even irrelevant ones)\n",
        "    * Doesn’t assess model generalization\n",
        "\n",
        "19. **Interpret large standard error for a coefficient:**\n",
        "    The estimate is unstable or imprecise, possibly due to **multicollinearity** or **insufficient data**.\n",
        "\n",
        "20. **Identify heteroscedasticity in residual plots:**\n",
        "    Look for **funnel shapes** (residuals fan out). It’s important to address as it can lead to **biased standard errors**.\n",
        "\n",
        "21. **High R² but low adjusted R²:**\n",
        "    The model may be **overfitting** with too many non-informative predictors.\n",
        "\n",
        "22. **Why scale variables in MLR:**\n",
        "\n",
        "    * Helps with **interpretability**\n",
        "    * Necessary for **regularization** (e.g., Ridge, Lasso)\n",
        "    * Avoids bias toward variables with larger ranges\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "23. **What is Polynomial Regression?**\n",
        "    Regression that models a nonlinear relationship by introducing polynomial terms (e.g., X², X³).\n",
        "\n",
        "24. **Difference from linear regression:**\n",
        "    Linear regression fits a straight line, polynomial regression fits a **curved line**.\n",
        "\n",
        "25. **When to use polynomial regression:**\n",
        "    When the data shows a **non-linear trend** that can't be captured by a straight line.\n",
        "\n",
        "26. **General equation for polynomial regression:**\n",
        "\n",
        "    $$\n",
        "    Y = b_0 + b_1X + b_2X^2 + b_3X^3 + \\dots + b_nX^n\n",
        "    $$\n",
        "\n",
        "27. **Can it be applied to multiple variables?**\n",
        "    Yes, this becomes **Multivariate Polynomial Regression** (e.g., terms like $X_1^2, X_1X_2$).\n",
        "\n",
        "28. **Limitations of polynomial regression:**\n",
        "\n",
        "    * Overfitting (especially at high degrees)\n",
        "    * Poor extrapolation\n",
        "    * Sensitive to outliers\n",
        "\n",
        "29. **Model fit methods for selecting polynomial degree:**\n",
        "\n",
        "    * **Adjusted R²**\n",
        "    * **Cross-validation (CV)**\n",
        "    * **AIC/BIC (information criteria)**\n",
        "    * **Residual plots**\n",
        "\n",
        "30. **Why visualization is important in polynomial regression:**\n",
        "    To verify **fit quality**, detect **overfitting**, and understand **nonlinear trends**.\n",
        "\n",
        "31. **How to implement polynomial regression in Python:**\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Sample implementation for 3rd degree\n",
        "model = make_pipeline(PolynomialFeatures(degree=3), LinearRegression())\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6c0uMy5XAuKm"
      }
    }
  ]
}